{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import Classifier\n",
    "from dataloader import FairnessDataset\n",
    "from algorithm import train_fair_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Which dataset to test #####\n",
    "dataset_name = 'COMPAS' # ['Moon', 'Lawschool', 'AdultCensus', 'CreditDefault', 'COMPAS']\n",
    "\n",
    "##### Which fairness notion to consider (Demographic Parity / Equalized Odds) #####\n",
    "fairness = 'DP' # ['DP', 'EO']\n",
    "\n",
    "##### Model specifications #####\n",
    "n_layers = 2 # [positive integers]\n",
    "n_hidden_units = 16 # [positive integers]\n",
    "\n",
    "##### Our algorithm hyperparameters #####\n",
    "h = 0.1 # Bandwidth hyperparameter in KDE [positive real numbers]\n",
    "delta = 1.0 # Delta parameter in Huber loss [positive real numbers]\n",
    "lambda_ = 0.05 # regularization factor of DDP/DEO; Positive real numbers \\in [0.0, 1.0]\n",
    "\n",
    "##### Other training hyperparameters #####\n",
    "batch_size = 2048\n",
    "lr = 2e-4\n",
    "lr_decay = 1.0 # Exponential decay factor of LR scheduler\n",
    "n_seeds = 5 # Number of random seeds to try\n",
    "n_epochs = 200\n",
    "\n",
    "##### Whether to enable GPU training or not\n",
    "device = torch.device('cuda') # or torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "starting_time = time.time()\n",
    "\n",
    "seed = 0\n",
    "IPython.display.clear_output()\n",
    "print('Currently working on - seed: {}'.format(seed))\n",
    "\n",
    "# Set a seed for random number generation\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Import dataset\n",
    "dataset = FairnessDataset(dataset=dataset_name, device=device)\n",
    "dataset.normalize()\n",
    "input_dim = dataset.XZ_train.shape[1]\n",
    "\n",
    "# Create a classifier model\n",
    "net = Classifier(n_layers=n_layers, n_inputs=input_dim, n_hidden_units=n_hidden_units)\n",
    "net = net.to(device)\n",
    "\n",
    "# Set an optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay) # None\n",
    "for lambda_ in np.logspace(-2, 2, 50):\n",
    "# Fair classifier training\n",
    "    temp = train_fair_classifier(dataset=dataset, \n",
    "                                 net=net, \n",
    "                                 optimizer=optimizer, lr_scheduler=lr_scheduler,\n",
    "                                 fairness=fairness, lambda_=lambda_, h=h, delta=delta, \n",
    "                                 device=device, n_epochs=n_epochs, batch_size=batch_size, seed=seed)\n",
    "    temp['seed'] = seed\n",
    "    result = result.append(temp)\n",
    "\n",
    "print('Average running time: {:.3f}s'.format((time.time() - starting_time) / 5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
